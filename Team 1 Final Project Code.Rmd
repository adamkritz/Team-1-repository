---
title: "Team 1 Final Project Code"
author: "Team 1 (Names: Adam Kritz, Osemekhian Ehilen, Suhas Buravalla, Huang He)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---



```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
```

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# Midterm Code

## Importing the Data

```{r read the dataset}
diamonds = data.frame(read.csv("diamonds.csv"))
diamonds$color = factor(diamonds$color, order=T, levels = c('J', 'I', 'H', 'G', 'F', 'E', 'D'))
diamonds$clarity = factor(diamonds$clarity, order=T, levels = c('I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'))
diamonds$cut = factor(diamonds$cut, order=T, levels = c('Fair', 'Good', 'Very Good', 'Premium', 'Ideal'))
colnames(diamonds)[1] = "ID"
```

## EDA Code

### summary table

```{r, results = 'markup'}
sum = xkablesummary(diamonds, title = "Summary of Diamonds")
sum
```

### Categorical Data Exploration

```{r Color Exploration}
barplot(table(diamonds$color), col = c(2, 3, 4, 7, 6, 8, 9), main = 'Barplot of Color', xlab = "Color", ylab = "Frequency")
```


color plot

```{r Clarity Exploration}
barplot(table(diamonds$clarity), col = c(2, 3, 4, 7, 6, 8, 9, 5), main = 'Barplot of Clarity', xlab = "Clarity", ylab = "Frequency")
```
clarity plot

```{r Cut Exploration}
barplot(table(diamonds$cut), col = c(6, 7, 8, 3, 4), main = 'Barplot of Cut', xlab = "Cut", ylab = "Frequency")

cut_count = dplyr::count(diamonds, cut)
colnames(cut_count)[colnames(cut_count) == 'cut'] <- 'Cut'
cut_count$n = cut_count$n/sum(cut_count$n)

loadPkg("ggplot2")

ggplot(cut_count, aes(x = "", y = n, fill = Cut)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  geom_text(size = 4, aes(label = paste0(round(n, 2), "")), position = position_stack(vjust = 0.5))+
  scale_fill_manual(values = c(6, 7, 8, 3, 4))+
  labs(title='Pie Chart of Cut')
```
cut plot

### Continious Data Exploration

In this section we will look at all the continuous variables besides price.

```{r Length, Size, Depth Exploration}
diaNO = subset(diamonds, x > 0 & y > 0 & z > 0 & y <30 & z <30)
ggplot(diaNO, aes(x, y, color=z)) +
  geom_point(size = 8, alpha = 0.6) +
  ggtitle('Y versus X versus Z for Diamonds (Outliers Removed)') +
  scale_color_gradient2(midpoint = 5, low="blue", mid="green", high="red")
```

x vs y vs z


```{r Table Exploration}
ggplot(diamonds, aes(y=table)) + 
  geom_histogram(col="black", 
                 fill="green", 
                  alpha = .8,
                 binwidth = 1.5) +
  labs(title='Histogram of Table') +
  coord_flip() +
  labs(y="Table", x="Frequency")
```

table plot

```{r Depth Exploration}
ggplot(diamonds, aes(y=depth)) + 
  geom_histogram(col="black", 
                 fill="red", 
                  alpha = .8,
                 binwidth = 1) +
  coord_flip() +
  labs(title='Histogram of Depth') +
  labs(y="Depth", x="Frequency")
```

depth plot

```{r Carat Exploration}
ggplot(diamonds, aes(y=carat)) + 
  geom_boxplot() + 
  geom_boxplot(colour="black", fill="orange", outlier.colour="orange", outlier.size=5, alpha = 0.8) +
  labs(title = 'Boxplot of Carat')+
  labs(y="Carat")
```

carat plot

### Price Exploration

```{r Price Exploration}
ggplot(diamonds, aes(y=price)) + 
  geom_boxplot() + 
  geom_boxplot(colour="black", fill="blue", outlier.colour="blue", outlier.size=5, alpha = 0.8) +
  labs(title = 'Boxplot of Price')

ggplot(diamonds, aes(y=price)) + 
  geom_histogram(col="black", 
                 fill="blue", 
                  alpha = .8,
                 binwidth = 300) +
  coord_flip() +
  labs(title='Histogram of Price') +
  labs(y="Price", x="Frequency")

qqnorm(diamonds$price, main="Q-Q plot of Price", col = 'blue', ylab = 'Price') 
qqline(diamonds$price)

```



price plot



## Adam - Linear Regression to predict price



### single variables

one model for each individual

``` {r single var models}
modelcarat <- lm(price ~ carat,data=diamonds)
modeldepth <- lm(price ~ depth,data=diamonds)
modeltable <- lm(price ~ table,data=diamonds)
modelx <- lm(price ~ x,data=diamonds)
modely <- lm(price ~ y,data=diamonds)
modelz <- lm(price ~ z,data=diamonds)

```

``` {r single var models 2, results = TRUE}
loadPkg("sjPlot")
loadPkg("sjmisc")
loadPkg("sjlabelled")
tab_model(modelcarat)
```

``` {r single var models 2.01, results = TRUE}
tab_model(modeldepth)
```

``` {r single var models 2.02, results = TRUE}
tab_model(modeltable)
```

``` {r single var models 2.03, results = TRUE}
tab_model(modelx)
```

``` {r single var models 2.04, results = TRUE}
tab_model(modely)
```

``` {r single var models 2.05, results = TRUE}
tab_model(modelz)
```

ANOVA of models

``` {r single var models 2.5, res, results = 'markup'}
anovaRes = anova(modelcarat,modelx,modely,modelz)
xkabledply(anovaRes, title = "ANOVA comparison between the models")
``` 


``` {r single var models 3}
loadPkg("modelr")
df.with.predfin.from.model <- add_predictions(diamonds,modelcarat)
head(df.with.predfin.from.model)
loadPkg("ggplot2")
ggplot(df.with.predfin.from.model,aes(price,pred))+geom_point(aes(price,pred), alpha =0.2)+geom_line(aes(pred), colour="red", size=1) + labs(title = "Carat Linear Model to Predict Price", x = "Price", y = "Prediction") + xlim(c(0, 20000))

loadPkg("car")
avPlots(modelcarat)
```

Carat is the best singular model

### multinumeric variables


``` {r corrplot}
diamondsnum =  subset(diamonds, select = c("price", "carat", "depth", "table", "x", "y", "z"))
diamondscor = cor(diamondsnum) 
loadPkg("corrplot")
corrplot.mixed(diamondscor)
```

high correlation between many variables

``` {r pairs plot}
#loadPkg("lattice") 
#pairs(diamondsnum[1:4])
```

removed the pairs plot for now cause I did not use in presentation and it takes too long to run

``` {r multinumeric models, results = TRUE}
modelcaratx <- lm(price ~ carat+x,data=diamonds)
tab_model(modelcaratx)
xkablevif(modelcaratx)
```

``` {r multinumeric models2, results = TRUE}
modelcarattable <- lm(price ~ carat+table,data=diamonds)
tab_model(modelcarattable)
xkablevif(modelcarattable)
```

``` {r multinumeric models3, results = TRUE}
modeldepthx <- lm(price ~ depth+x,data=diamonds)
tab_model(modeldepthx)
xkablevif(modeldepthx)
```
Every multinumeric model is either a worse predictor, or has higher vif values

There is no way to fix this from looking online


### multi numeric+continious variables

trying out different models

``` {r multi numeric+conitnious models, results = TRUE}
modelcaratcut <- lm(price ~ carat+cut, data = diamonds)
tab_model(modelcaratcut)
xkablevif(modelcaratcut)
``` 

``` {r multi numeric+conitnious models2, results = TRUE}
modelcaratclarity <- lm(price ~ carat+clarity, data = diamonds)
tab_model(modelcaratclarity)
xkablevif(modelcaratclarity)
```

This model is the best

``` {r multi numeric+conitnious models3, results = TRUE}
modelcaratcolor <- lm(price ~ carat+color, data = diamonds)
tab_model(modelcaratcolor)
xkablevif(modelcaratcolor)
```

``` {r multi numeric+conitnious models4, results = TRUE}
modelcaratccc <- lm(price ~ carat+color+cut+clarity, data = diamonds)
tab_model(modelcaratccc)
xkablevif(modelcaratccc)
```

Now this model is the best

``` {r caratccc plot}
df.with.predfin.from.model <- add_predictions(diamonds,modelcaratccc)
head(df.with.predfin.from.model)
loadPkg("ggplot2")
ggplot(df.with.predfin.from.model,aes(price,pred))+geom_point(aes(price,pred), alpha =0.2)+geom_line(aes(pred), colour="red", size=1) + labs(title = "Carat + Color + Cut + Clarity Linear Model", x = "Price", y = "Prediction") + xlim(c(0, 20000))
```

almost looks like a polynomial curve?

### interaction terms

``` {r interaction terms, results = TRUE}
modelcaratccci <- lm(price ~ carat * clarity, data = diamonds)
tab_model(modelcaratccci)
xkablevif(modelcaratccci)
```

pretty good model

``` {r interaction terms2, results = TRUE}
modelcaratccci2 <- lm(price ~ carat + (color+cut+clarity)^2, data = diamonds)
tab_model(modelcaratccci2)
xkablevif(modelcaratccci2)
```

high VIF values

``` {r interaction terms3, results = TRUE}
modelcaratccci3 <- lm(price ~ (carat + clarity)^2, data = diamonds)
tab_model(modelcaratccci3)
xkablevif(modelcaratccci3)
```

pretty great again, but high VIF


Nothing with interaction terms beats the carat + ccc model

### feature selection

```{r ft, results = TRUE}
loadPkg("leaps")
#This is essentially best fit 
reg.best10 <- regsubsets(price~. , data = diamonds, nvmax = 10, nbest = 1, method = "exhaustive")  # leaps::regsubsets() - Model selection by exhaustive (default) search, forward or backward stepwise, or sequential replacement
#The plot will show the Adjust R^2 when using the variables across the bottom
plot(reg.best10, scale = "adjr2", main = "Exhaustive Adjusted R^2")
```

all types of models look the same, so just stick to adjusted R squared

```{r feature selection 2}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "forward")
plot(reg.forward10, scale = "adjr2", main = "Forward Adjusted R^2")
# summary(reg.forward10)
```


again all the same


```{r feature selection 3}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "backward")
plot(reg.forward10, scale = "adjr2", main = "Backward Adjusted R^2")
# summary(reg.forward10)
```

same


```{r feature selection 4}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "seqrep")
plot(reg.forward10, scale = "adjr2", main = "Sequential Replacement Adjusted R^2")
# summary(reg.forward10)
```

same

All feature selections reflects our findings, and is all the same basically



## Suhas - PCR modeling to predict price


```{r, results='markup'}
diamondscont <- subset(diamonds, select = c ("carat", "depth", "table", "price", "x", "y", "z"))
```


#Standardize the data
```{r, results='markup'}
diamondscont <- as.data.frame(scale(diamondscont))
```

# Conduct PCR 

```{r, results='markup'}
library(pls)
pcr_model <- pcr(price~., data = diamondscont,ncomp=6, validation = "CV")
summary(pcr_model)
validationplot(pcr_model)
```

We can see from the results of the PCR model that with just one component we can explain around 79 % of variation in price and as the components increase, the variation in price explained increases very marginally so it would make sense to build a model with just one variable. 



# Linear regression model with 1 component
```{r}
linear_model_pcr_1a = lm(price~ x   , data= diamondscont)
summary(linear_model_pcr_1a)
```

```{r}
linear_model_pcr_1b = lm(price~ y   , data= diamondscont)
summary(linear_model_pcr_1b)
```

```{r}
linear_model_pcr_1c = lm(price~ z   , data= diamondscont)
summary(linear_model_pcr_1c)
```

```{r}
linear_model_pcr_1d = lm(price~ depth   , data= diamondscont)
summary(linear_model_pcr_1d)
```

```{r}
linear_model_pcr_1e = lm(price~ table   , data= diamondscont)
summary(linear_model_pcr_1e)
```

```{r}
linear_model_pcr_1f = lm(price~ carat   , data= diamondscont)
summary(linear_model_pcr_1f)
```

We can see from the above models that if we have to choose one variable to best explain the variation it price, it has to be the carat of the diamond because it has the highest adjusted r squared value among the other variables.


If we try to build a model with more than one variable as above, we can see that the adjusted r squared value of the model increases very slightly, which means that the variation in price is explained only a little better than the model with just one variable. If we see the vif values which checks the collinearity of the variable, we can see that all the continuous variables are highly collinear.


## Solomon - Random Forest Classification to predict cut

In this section we will be using the Random Forest model to build a classification model to be able to effectively classify diamonds into 'Fair', 'Good', 'Very Good', 'Premium', 'Ideal' diamond cuts, using the base features.

Prior to Random Forest Classification, Multi-nomial Logistic regression had been used but a very low accuracy score was found.

We start by removing missing values and outliers to make a good classification.

```{r preprocessing}
mydiamonds=diamonds
mydiamonds[ mydiamonds == "?"] <- NA
colSums(is.na(mydiamonds))
# str(mydiamonds)
# boxplot(mydiamonds$price)
# any(is.na.data.frame(mydiamonds_clean))
mydiamonds_clean= subset(mydiamonds,!mydiamonds$price %in% boxplot.stats(mydiamonds$price)$out)
```

Upon removing outliers, `r nrow(mydiamonds)- nrow(mydiamonds_clean)` outliers were detected and removed.

Next, we split our data set into training and testing set with 80% and 20% respectively.

```{r train_test_split}
library(randomForest)
require(caTools)
library(MASS)
library(caret)
set.seed(1)
dataset= mydiamonds_clean[,2:11]
test= createDataPartition(dataset$cut,p=.2, list= FALSE)
data_train= dataset[-test,]
data_test= dataset[test,]
```

### Multinomial Logistic Regression

Before we dive into Random Forest Regression, lets perform a multinomial Logistic Regression and take a look at the accuracy score.

```{r logisticReg}
require(nnet)
# Training the multinomial model
multinom_model <- multinom(cut ~ ., data = data_test)

# Checking the model
summary(multinom_model)

# Predicting the values for train dataset
classPredicted <- predict(multinom_model, newdata = data_test, "class")

# Confusion matrix
library(caret)
cmlogit=confusionMatrix(data_test$cut,classPredicted)
```

Logistic Regression has an accuracy score of `r round((sum(diag(cmlogit$table))/sum(cmlogit$table))*100,2)`%.

### Random Forest Regression

With Logsitic regression performing poorly, we will test Random Forest Regression.

First, we tune the Random Forest Model to get the best `mtry` (number of variables sampled at each split).

```{r tuningRF, results='markup'}
# Tuning Random Forest to get the best mtry (number of variables random sampled as candidates at each split)
tune= tuneRF(data_train[,-2],data_train[,2], stepFactor = 0.5, plot=TRUE,
             ntreeTry=75,trace=TRUE, improve=0.05)

```

Note: `mtry` is the number of predictors sampled for splitting at each node.

The chart above helps us choose the best `mtry` to get a minimal Out of Bag Error. Obviously, 6 is the best `mtry` with less than 22% OOB error.


```{r rfc, results='markup'}

rf<- randomForest(cut~ .,data=data_train,mtry=6, ntree=75) #fitting RandomForest Classification
pred = predict(rf, newdata=data_test[,-2]) # Prediction on test set
cm=confusionMatrix(data_test$cut,pred) #confusion matrix
print(rf)
```

The random forest classifier has been modeled with mtry as 6 and ntree as 75..

```{r, results='markup'}
xkabledply(cm$table,title="Confusion Matrix")
cm$overall
```

The confusion matrix is seen above. Also, we get an accuracy score of `r cm$overall[1]*100`% for the test dataset.

```{r,results='markup'}
plot(rf)
```

The graph above shows that as the trees increases the Out of Bag Error reduces. While a larger size of trees greater than 75 does not reduce the out of bag error significantly.

```{r importance,results='markup'}
round(importance(rf), 2)
```

```{r, results='markup'}
# Mean Decrease in Gini is the average (mean) of a variable's total decrease in node impurity
varImpPlot(rf)
```

The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease Gini score, the higher the importance of the variable in the model.

In the above chart, `table` and `depth` have the highest importance in the Random Forest model.

```{r}
set.seed(1)
dataset2= mydiamonds_clean[,c(2,3,4,6,7,8,9,10,11)]
test2= createDataPartition(dataset2$cut,p=.2, list= FALSE)
data_train2= dataset2[-test2,]
data_test2= dataset2[test2,]

rf2<- randomForest(cut~ .,data=data_train2,mtry=6, ntree=75) #fitting RandomForest Classification
pred2 = predict(rf2, newdata=data_test2[,-2]) # Prediction on test set
cm2=confusionMatrix(data_test2$cut,pred2) #confusion matrix
print(rf2)
```

Removing least mean decrease gini feature (clarity) increases the Out of Bag error by 0.1%. We will stick to the first random forest model for predicting diamond cut type with minimal out of bag error. 

```{r check,results='markup'}
check= data.frame()
# c('accuracy','precision','sensitivity','specificity')
check=rbind(check,c(round((sum(diag(cmlogit$table))/sum(cmlogit$table))*100,2)))
check=rbind(check,c(round((sum(diag(cm$table))/sum(cm$table))*100,2)))
rownames(check)=c('logistic','random forest')
colnames(check)=c("Accuracy Score")
check$AverageSensitiviy=c((sum(cmlogit$byClass[1:5])/5)*100,(sum(cm$byClass[1:5])/5)*100)
check$AverageSpecificity=c((sum(cmlogit$byClass[6:10])/5)*100,(sum(cm$byClass[6:10])/5)*100)
check$AveragePrecision=c((sum(cmlogit$byClass[21:25])/5)*100,(sum(cm$byClass[21:25])/5)*100)

barplot(height=as.matrix(check),beside=TRUE,names.arg = c("Accuracy","Sensitivity","Specificity","Precision"),legend.text = c("Logistic","RandoForest"),col=c("blue","green"))

```

The barplot above gives us a pictorial view of how Random Forest surpasses Logistic regression for classifying diamond cut type.


## Huang - KNN modeling to predict cut

# pick numeric columns only
```{r}
knndata = diamonds[,c(2,3,6,7,8,9,10,11)]
knndata = as.data.frame(scale(knndata[,c(1,3,4,5,6,7,8)], center = TRUE, scale = TRUE))
```

# split data set
```{r}
set.seed(1000)
knn_sample <- sample(2, nrow(knndata), replace=TRUE, prob=c(0.75, 0.25))
knn_training <- knndata[knn_sample==1, ]
knn_test <- knndata[knn_sample==2, ]
trainLabels <- diamonds[knn_sample==1, 3]
testLabels <- diamonds[knn_sample==2, 3]
```

# build KNN model with K = 7
```{r}
loadPkg("FNN")
loadPkg("gmodels")
loadPkg("caret")
knn_pred <- knn(train = knn_training, test = knn_test, cl=trainLabels, k=7)
KNNCross <- CrossTable(testLabels, knn_pred, prop.chisq = FALSE)
```

# check different k values
```{r}
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
for (kval in 3:15) {
  knn_pred <- knn(train = knn_training, test = knn_test, cl=trainLabels, k=kval)
  KNNCross <- CrossTable(testLabels, knn_pred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  KNNCross
  # get confusion matrix
  cm = confusionMatrix(knn_pred, reference = testLabels ) 
  # get accuracy score
  cmaccu = cm$overall['Accuracy']
  print( paste("Total Accuracy = ", cmaccu ) )
  # store into dataframe
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL )
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
```

# show different accuracy values
```{r}
xkabledply(ResultDf, "Total Accuracy Summary")
```

# draw "accuracy vs k" plot
```{r}
loadPkg("ggplot2")
ggplot(ResultDf,
       aes(x = k, y = Total.Accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "accuracy vs k")
```


```{r}

```


