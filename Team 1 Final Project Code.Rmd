---
title: "Team 1 Final Project Code"
author: "Team 1 (Names: Adam Kritz, Osemekhian Ehilen, Suhas Buravalla, Huang He)"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---



```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
```

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# Midterm Code

## Importing the Data

```{r read the dataset}
diamonds = data.frame(read.csv("diamonds.csv"))
diamonds$color = factor(diamonds$color, order=T, levels = c('J', 'I', 'H', 'G', 'F', 'E', 'D'))
diamonds$clarity = factor(diamonds$clarity, order=T, levels = c('I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'))
diamonds$cut = factor(diamonds$cut, order=T, levels = c('Fair', 'Good', 'Very Good', 'Premium', 'Ideal'))
colnames(diamonds)[1] = "ID"
```

## EDA Code

### summary table

```{r, results = 'markup'}
sum = xkablesummary(diamonds, title = "Summary of Diamonds")
sum
```

### Categorical Data Exploration

```{r Color Exploration}
barplot(table(diamonds$color), col = c(2, 3, 4, 7, 6, 8, 9), main = 'Barplot of Color', xlab = "Color", ylab = "Frequency")
```


color plot

```{r Clarity Exploration}
barplot(table(diamonds$clarity), col = c(2, 3, 4, 7, 6, 8, 9, 5), main = 'Barplot of Clarity', xlab = "Clarity", ylab = "Frequency")
```
clarity plot

```{r Cut Exploration}
barplot(table(diamonds$cut), col = c(6, 7, 8, 3, 4), main = 'Barplot of Cut', xlab = "Cut", ylab = "Frequency")

cut_count = dplyr::count(diamonds, cut)
colnames(cut_count)[colnames(cut_count) == 'cut'] <- 'Cut'
cut_count$n = cut_count$n/sum(cut_count$n)

loadPkg("ggplot2")

ggplot(cut_count, aes(x = "", y = n, fill = Cut)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  geom_text(size = 4, aes(label = paste0(round(n, 2), "")), position = position_stack(vjust = 0.5))+
  scale_fill_manual(values = c(6, 7, 8, 3, 4))+
  labs(title='Pie Chart of Cut')
```
cut plot

### Continious Data Exploration

In this section we will look at all the continuous variables besides price.

```{r Length, Size, Depth Exploration}
diaNO = subset(diamonds, x > 0 & y > 0 & z > 0 & y <30 & z <30)
ggplot(diaNO, aes(x, y, color=z)) +
  geom_point(size = 8, alpha = 0.6) +
  ggtitle('Y versus X versus Z for Diamonds (Outliers Removed)') +
  scale_color_gradient2(midpoint = 5, low="blue", mid="green", high="red")
```

x vs y vs z


```{r Table Exploration}
ggplot(diamonds, aes(y=table)) + 
  geom_histogram(col="black", 
                 fill="green", 
                  alpha = .8,
                 binwidth = 1.5) +
  labs(title='Histogram of Table') +
  coord_flip() +
  labs(y="Table", x="Frequency")
```

table plot

```{r Depth Exploration}
ggplot(diamonds, aes(y=depth)) + 
  geom_histogram(col="black", 
                 fill="red", 
                  alpha = .8,
                 binwidth = 1) +
  coord_flip() +
  labs(title='Histogram of Depth') +
  labs(y="Depth", x="Frequency")
```

depth plot

```{r Carat Exploration}
ggplot(diamonds, aes(y=carat)) + 
  geom_boxplot() + 
  geom_boxplot(colour="black", fill="orange", outlier.colour="orange", outlier.size=5, alpha = 0.8) +
  labs(title = 'Boxplot of Carat')+
  labs(y="Carat")
```

carat plot

### Price Exploration

```{r Price Exploration}
ggplot(diamonds, aes(y=price)) + 
  geom_boxplot() + 
  geom_boxplot(colour="black", fill="blue", outlier.colour="blue", outlier.size=5, alpha = 0.8) +
  labs(title = 'Boxplot of Price')

ggplot(diamonds, aes(y=price)) + 
  geom_histogram(col="black", 
                 fill="blue", 
                  alpha = .8,
                 binwidth = 300) +
  coord_flip() +
  labs(title='Histogram of Price') +
  labs(y="Price", x="Frequency")

qqnorm(diamonds$price, main="Q-Q plot of Price", col = 'blue', ylab = 'Price') 
qqline(diamonds$price)

```



price plot



## Adam - Linear Regression to predict price



### single variables

one model for each individual

``` {r single var models}
modelcarat <- lm(price ~ carat,data=diamonds)
modeldepth <- lm(price ~ depth,data=diamonds)
modeltable <- lm(price ~ table,data=diamonds)
modelx <- lm(price ~ x,data=diamonds)
modely <- lm(price ~ y,data=diamonds)
modelz <- lm(price ~ z,data=diamonds)

```

``` {r single var models 2, results = TRUE}
loadPkg("sjPlot")
loadPkg("sjmisc")
loadPkg("sjlabelled")
tab_model(modelcarat)
```

``` {r single var models 2.01, results = TRUE}
tab_model(modeldepth)
```

``` {r single var models 2.02, results = TRUE}
tab_model(modeltable)
```

``` {r single var models 2.03, results = TRUE}
tab_model(modelx)
```

``` {r single var models 2.04, results = TRUE}
tab_model(modely)
```

``` {r single var models 2.05, results = TRUE}
tab_model(modelz)
```

ANOVA of models

``` {r single var models 2.5, res, results = 'markup'}
anovaRes = anova(modelcarat,modelx,modely,modelz)
xkabledply(anovaRes, title = "ANOVA comparison between the models")
``` 


``` {r single var models 3}
loadPkg("modelr")
df.with.predfin.from.model <- add_predictions(diamonds,modelcarat)
head(df.with.predfin.from.model)
loadPkg("ggplot2")
ggplot(df.with.predfin.from.model,aes(price,pred))+geom_point(aes(price,pred), alpha =0.2)+geom_line(aes(pred), colour="red", size=1) + labs(title = "Carat Linear Model to Predict Price", x = "Price", y = "Prediction") + xlim(c(0, 20000))

loadPkg("car")
avPlots(modelcarat)
```

Carat is the best singular model

### multinumeric variables


``` {r corrplot}
diamondsnum =  subset(diamonds, select = c("price", "carat", "depth", "table", "x", "y", "z"))
diamondscor = cor(diamondsnum) 
loadPkg("corrplot")
corrplot.mixed(diamondscor)
```

high correlation between many variables

``` {r pairs plot}
#loadPkg("lattice") 
#pairs(diamondsnum[1:4])
```

removed the pairs plot for now cause I did not use in presentation and it takes too long to run

``` {r multinumeric models, results = TRUE}
modelcaratx <- lm(price ~ carat+x,data=diamonds)
tab_model(modelcaratx)
xkablevif(modelcaratx)
```

``` {r multinumeric models2, results = TRUE}
modelcarattable <- lm(price ~ carat+table,data=diamonds)
tab_model(modelcarattable)
xkablevif(modelcarattable)
```

``` {r multinumeric models3, results = TRUE}
modeldepthx <- lm(price ~ depth+x,data=diamonds)
tab_model(modeldepthx)
xkablevif(modeldepthx)
```
Every multinumeric model is either a worse predictor, or has higher vif values

There is no way to fix this from looking online


### multi numeric+continious variables

trying out different models

``` {r multi numeric+conitnious models, results = TRUE}
modelcaratcut <- lm(price ~ carat+cut, data = diamonds)
tab_model(modelcaratcut)
xkablevif(modelcaratcut)
``` 

``` {r multi numeric+conitnious models2, results = TRUE}
modelcaratclarity <- lm(price ~ carat+clarity, data = diamonds)
tab_model(modelcaratclarity)
xkablevif(modelcaratclarity)
```

This model is the best

``` {r multi numeric+conitnious models3, results = TRUE}
modelcaratcolor <- lm(price ~ carat+color, data = diamonds)
tab_model(modelcaratcolor)
xkablevif(modelcaratcolor)
```

``` {r multi numeric+conitnious models4, results = TRUE}
modelcaratccc <- lm(price ~ carat+color+cut+clarity, data = diamonds)
tab_model(modelcaratccc)
xkablevif(modelcaratccc)
```

Now this model is the best

``` {r caratccc plot}
df.with.predfin.from.model <- add_predictions(diamonds,modelcaratccc)
head(df.with.predfin.from.model)
loadPkg("ggplot2")
ggplot(df.with.predfin.from.model,aes(price,pred))+geom_point(aes(price,pred), alpha =0.2)+geom_line(aes(pred), colour="red", size=1) + labs(title = "Carat + Color + Cut + Clarity Linear Model", x = "Price", y = "Prediction") + xlim(c(0, 20000))
```

almost looks like a polynomial curve?

### interaction terms

``` {r interaction terms, results = TRUE}
modelcaratccci <- lm(price ~ carat * clarity, data = diamonds)
tab_model(modelcaratccci)
xkablevif(modelcaratccci)
```

pretty good model

``` {r interaction terms2, results = TRUE}
modelcaratccci2 <- lm(price ~ carat + (color+cut+clarity)^2, data = diamonds)
tab_model(modelcaratccci2)
xkablevif(modelcaratccci2)
```

high VIF values

``` {r interaction terms3, results = TRUE}
modelcaratccci3 <- lm(price ~ (carat + clarity)^2, data = diamonds)
tab_model(modelcaratccci3)
xkablevif(modelcaratccci3)
```

pretty great again, but high VIF


Nothing with interaction terms beats the carat + ccc model

### feature selection

```{r ft, results = TRUE}
loadPkg("leaps")
#This is essentially best fit 
reg.best10 <- regsubsets(price~. , data = diamonds, nvmax = 10, nbest = 1, method = "exhaustive")  # leaps::regsubsets() - Model selection by exhaustive (default) search, forward or backward stepwise, or sequential replacement
#The plot will show the Adjust R^2 when using the variables across the bottom
plot(reg.best10, scale = "adjr2", main = "Exhaustive Adjusted R^2")
```

all types of models look the same, so just stick to adjusted R squared

```{r feature selection 2}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "forward")
plot(reg.forward10, scale = "adjr2", main = "Forward Adjusted R^2")
# summary(reg.forward10)
```


again all the same


```{r feature selection 3}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "backward")
plot(reg.forward10, scale = "adjr2", main = "Backward Adjusted R^2")
# summary(reg.forward10)
```

same


```{r feature selection 4}
reg.forward10 <- regsubsets(price~., data = diamonds, nvmax = 10, nbest = 1, method = "seqrep")
plot(reg.forward10, scale = "adjr2", main = "Sequential Replacement Adjusted R^2")
# summary(reg.forward10)
```

same

All feature selections reflects our findings, and is all the same basically



## Suhas - PCR modeling to predict price


```{r, results='markup'}
diamondscont <- subset(diamonds, select = c ("carat", "depth", "table", "price", "x", "y", "z"))
```


#Standardize the data
```{r, results='markup'}
diamondscont <- as.data.frame(scale(diamondscont))
```

# Conduct PCR 

```{r, results='markup'}
library(pls)
pcr_model <- pcr(price~., data = diamondscont,ncomp=6, validation = "CV")
summary(pcr_model)
```

```{r, results='markup'}
# We can see from the results of the PCR model that with just one component we can explain around 79 % of variance in price and as the components increase, the variance in price explained increases very marginally so it would make sense to build a model with just one variable. 
```


# Linear regression model with 1 component
```{r}
linear_model_pcr_1a = lm(price~ x   , data= diamondscont)
summary(linear_model_pcr_1a)
```

```{r}
linear_model_pcr_1b = lm(price~ y   , data= diamondscont)
summary(linear_model_pcr_1b)
```

```{r}
linear_model_pcr_1c = lm(price~ z   , data= diamondscont)
summary(linear_model_pcr_1c)
```

```{r}
linear_model_pcr_1d = lm(price~ depth   , data= diamondscont)
summary(linear_model_pcr_1d)
```

```{r}
linear_model_pcr_1e = lm(price~ table   , data= diamondscont)
summary(linear_model_pcr_1e)
```

```{r}
linear_model_pcr_1f = lm(price~ carat   , data= diamondscont)
summary(linear_model_pcr_1f)
```


# Linear regression model with 2 components 
```{r}
linear_model_pcr_2 = lm(price~ carat + x  , data= diamondscont)
summary(linear_model_pcr_2)
```




## Solomon - Random Forest to predict cut



## Huang - KNN modeling to predict cut



